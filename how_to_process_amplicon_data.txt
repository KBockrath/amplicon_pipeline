#################################################################################
# How to process amplicon data for Steps 1, 2, 3, and 4 outside of HTCondor DAG #
#################################################################################

# All scripts nescessary for analysis are located at:
# https://github.com/sgmccalla/amplicon_pipeline
# all of the step1, step2, step3, and step4 python/condor/shell/text files need to be in /home/smccalla (or your home directory i.e. /home/ylor)

#_____________________________________________________________________________________________________________

# Dockerfile located at:
# https://hub.docker.com/r/smccalla/amplicon_dockerfile/
# check to see if the docker image is on your computer:
docker images
# do you see "smccalla/amplicon_dockerfile" under the "REPOSITORY" heading?           
# if so, the docker image is ready to use. If not, execute:
docker pull smccalla/amplicon_dockerfile
# this will pull the docker image to your computer. May take ~ 45 min.

#_____________________________________________________________________________________________________________

# Other dependencies: 
# 1. The ncbi nr database must be downloaded to your local computer/server (~ 40 GB).
# 2. HTCondor must be installed on the local computer/server.
# 3. bcl2fastq must be installed on the local computer/server.
# 4. Illumina Experiment Manager on your local laptop (need user interface---not accessed using the terminal)
# 5. ncbi_taxonomy_expanded.tsv in your home directory (i.e. /home/smccalla) where you'll start Docker from (i.e. /home/smccalla/:/home/smccalla/)

#_____________________________________________________________________________________________________________

# Run bcl2fastq (version 2.18.0.12) on the NextSeq run if this has not been done already:
# 1. --runfolder-dir : directory where the raw data is located at
# 2. --output-dir : where the output will be written to
# 3. --sample-sheet : where the sample sheet made from Illumina Experiment Manager is located at
# 4. --interop-dir : 

nohup /usr/local/bin/bcl2fastq --runfolder-dir /home/koiuser/160408_NB501144_0002_AHMFMNBGXX/ --output-dir /home/koiuser/160408_NB501144_0002_AHMFMNBGXX/bcl2fastq_output --interop-dir /home/koiuser/160408_NB501144_0002_AHMFMNBGXX/ --sample-sheet /home/smccalla/data/170405_NB501053_0006_AHK2LGBGXY/SampleSheet.csv

# So in this example, the raw data will be located at: /home/smccalla/working_docker_directory/raw_data

#_____________________________________________________________________________________________________________

# STEP 1

# This step copies the raw data files to the current working directory, executes fastQValidator, executes FastQC,
# executes trimmomatic, joins paired ends

# From the command line, start docker:
docker run -it -v /home/smccalla/:/home/smccalla/ -w /usr/ -i smccalla/amplicon_dockerfile /bin/bash

# If you get the error 
# "docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. See 'docker run --help'." 
# run the following command before beginning Docker
systemctl start docker

# start qiime:
source activate qiime1

# change directory to the directory with the step1, 2, 3, and 4 python files:
# might not have to do this anymore since I added the steps to the docker file
cd /home/smccalla

# install biopython package:
conda install -q -y biopython
pip2 install biopython
# Installing biopython can be complicated sometimes because there are 2 versions of python in the docker container:
# Python2 for Qiime1 and Python3 for conda. So you have to be specific *WHERE* (meaning python2 or python3) you want biopython installed.

# change bash prompt to shorten (just an option, not necessary)
PS1="umesc : "

# If you know the sample prefix heading you ran before, you can batch change the heading name for all of the files with sed.
# The following command changes "GAIM_02" to "GAIM_03" in files step1.py step2.py step3.sub step3_outformat_6.sh step4.py
# and "GAIM03" to "GAIM05" in the file qiime_mapping_file_corrected.txt (the sample id cannot have underscores)
sed -i 's/GAIM_05/GAIM_12/g' step1.py step2.py step3.sub step3_outformat_6.sh step4.py
sed -i 's/GAIM05/GAIM12/g' qiime_mapping_file_corrected.txt

# If you want to change the analysis to a different sample library, change the "GAIM_15" to your library of interest, 
# i.e. change to "GAIM_16" in the command line arguments.
# There are 7 elements you need to start Step1.
# 1. Invoke python: python 
# 2. Call the python script you want to run: step1.py 
# 3. All of the forward sequences you want to analyze. The "*" is a wildcard here that will match everything with this pattern: GAIM_15*_R1_001.fastq.gz
# 4. All of the reverse sequences you want to analyze. The "*" is a wildcard here that will match everything with this pattern: GAIM_15*_R2_001.fastq.gz
# 5. The name of the library: GAIM_15
# 6. NextSeq run name: 160408_NB501144_0002_AHMFMNBGXX
# 7. bcl2fastq project name: Loons_Long_Tailed
# Enter all of those elements (1-7) all at once at the command line.

# To execute step1:
python step1.py GAIM_15*_R1_001.fastq.gz GAIM_15*_R2_001.fastq.gz GAIM_15 160408_NB501144_0002_AHMFMNBGXX Loons_Long_Tailed

# This will create a folder in "GAIM_15" called "data". This will have folders within it "fastQC", "fastQValidator", "join_paired_ends", "trimdata"
# and the data folder will also have all of the transferred and unzipped data in it.

#_____________________________________________________________________________________________________________

# STEP 2

# This step converts fasta to fastq, changes the sequence names, assigns taxonomy #1, splits the data into many files

# ***Make sure you have a mapping file and/or that the mapping file is correct (i.e. change "GAIM_15" to "GAIM_16" 
#    in loon_qiime_mapping_file_trainer_file_3_corrected.txt)***
# If you want to change the analysis to a different sample library, change the "GAIM_15" to your library of interest, 
# i.e. change to "GAIM_16" in the command line arguments.
# There are 3 elements you need to start Step2:
# 1. Invoke python: python 
# 2. Call the python script you want to run: step1.py 
# 3. The name of the library: GAIM_15

# To execute step2:
python step2.py GAIM_15

# This will create folders and files in /home/smccalla/GAIM_15/data like fasta_files/fastqjoin.join.fna.formatted/.
# and the large fasta data file will be chunked into many smaller files for Step3.

# If you get the following error it means biopython is not installed:
Traceback (most recent call last):
  File "step2.py", line 192, in <module>
    record_iter = SeqIO.parse(open(newotupath),"fasta")
NameError: name 'SeqIO' is not defined

# If needed, within docker, change the permissions for all of the files you just made since docker operates as root.
# The "-R" means recursively, so it changes the permissions of not just "GAIM_15", but all of the subfolders and files:
chmod -R 777 /home/smccalla/GAIM_15

#_____________________________________________________________________________________________________________

# STEP 3

# This step BLASTS the fasta files

# Exit docker:
exit

# Or open a new terminal window to execute Step 3.

# Change directory to where your scripts are, if necessary:
cd /home/smccalla/

# Make a "log" folder  for the condor output file in "/home/smccalla/test_28" ---> "/home/smccalla/GAIM_15/log"
mkdir /home/smccalla/GAIM_15/log

# Change the library name in both the step3.sub and step3_outformat_6.sh files, i.e. change all instances of "GAIM_15" to "GAIM_16"

# Use HTCondor to BLAST the fasta files in /home/smccalla/test_28/data/fasta_files/fastqjoin.join.fna.formatted/group_*.fasta
# that were split up from the combined_seqs.fna file into many smaller files.
condor_submit step3.sub

# You'll get the following warning, but it's fine. It's just because there's a wildcard ("*") in the queue command:
# queue filename matching files /home/smccalla/GAIM_15/data/fasta_files/fastqjoin.join.fna.formatted/group_*.fasta
WARNING: the Queue variable 'filename' was unused by condor_submit. Is it a typo?

# Check the status of the BLAST jobs by:
condor_q

# If you want to remove/cancel your HTCondor jobs, just substitute "smccalla" with your user name:
condor_rm smccalla 

#_____________________________________________________________________________________________________________

# STEP 4

# This step concatenates the BLAST results, assigns taxonomy #2, summarizes taxa, and summarizes taxa through plots
# executes trimmomatic, joins paired ends

# start docker:
docker run -it -v /home/smccalla/:/home/smccalla/ -w /usr/ -i smccalla/amplicon_dockerfile /bin/bash

# change directory to the directory with the step1, 2, 3, and 4 python files:
cd /home/smccalla

# start qiime:
source activate qiime1

# Install the biopython package.
# Installing biopython can be complicated sometimes because there are 2 versions of python in the docker container which are
# Python2 for Qiime1 and Python3 for conda. So you have to be specific *WHERE* (meaning python2 or python3) you want biopython installed.
conda install -q -y biopython

# change bash prompt to shorten (just an option, not necessary)
PS1="umesc : "

# Change the permissions for all of the files you just made.
chmod -R 777 /home/smccalla/GAIM_15

# If you want to change the analysis to a different sample library, change the "GAIM_15" to your library of interest, 
# i.e. change to "GAIM_16" in the command line arguments.
# There are 3 elements you need to start Step4.
# 1. Invoke python: python 
# 2. Call the python script you want to run: step4.py 
# 3. The name of the library: GAIM_15

# To execute step4:
python step4.py GAIM_15

# If you get an error like:
RuntimeError: Invalid DISPLAY variable
# It's a Qiime matplotlib error and you have to run the following commands:
# https://groups.google.com/forum/#!searchin/qiime-forum/.matplotlibrc/qiime-forum/6srHDScyh_0/CJeu_Xl4BgAJ
# https://groups.google.com/forum/#!searchin/qiime-forum/Plot$20Taxonomy$20Summary$20using$201$20sample$20%7Csort:relevance/qiime-forum/8Zkb5-Cnh8Y/T5S4Io60WhoJ
cd /opt/conda/envs/qiime1/lib/python2.7/site-packages/matplotlib/mpl-data/
sed -i 's/backend      : Qt4Agg/backend      : agg/g' matplotlibrc
sed -i 's/backend      : Qt4Agg/backend      : agg/g' /opt/conda/envs/qiime1/lib/python2.7/site-packages/matplotlib/mpl-data/matplotlibrc
cd /home/smccalla/

#_____________________________________________________________________________________________________________

# Post-processing steps:
# To free up space on the KOI, it is a good practice to move the processed data to the Nimble server after data analysis.

#_____________________________________________________________________________________________________________

# Extras that may be helpful:

# Merge two or more OTU tables into a single OTU table
# http://qiime.org/scripts/merge_otu_tables.html
merge_otu_tables.py -i otu_table1.biom,otu_table2.biom -o merged_otu_table.biom
merge_otu_tables.py -i GAIM01.biom,GAIM02.biom -o GAIM.01.02.merged_otu_table.biom

# The following commands may be peridocially useful for Docker since Docker can use up root memory dramatically:
# https://getintodevops.com/blog/keeping-the-whale-happy-how-to-clean-up-after-docker
# https://github.com/chadoe/docker-cleanup-volumes
# https://gist.github.com/bastman/5b57ddb3c11942094f8d0a97d461b430
# Clean up dead and exited Docker containers
# SEE DISK SPACE USAGE
df -h
# SEE LIST OF RUNNING CONTAINERS
docker ps
# SEE LIST OF STORED IMAGES
docker images
# Remove image based on IMAGE ID:
docker rmi -f 9138d58a1bc8
# DELETE ORPHANED AND DANGLING VOLUMES
docker volume rm $(docker volume ls -qf dangling=true)
# Dangling volumes are volumes that are not being used by a container. Dangling images are images that are not referenced to by containers or other images.
# DELETE DANGLING AND UNTAGGED IMAGES
docker rmi $(docker images -q -f dangling=true)
# Cleanup commands, if you know what you're doing OR like living on the edge
# DELETE EXITED CONTAINERS
docker rm $(docker ps -aqf status=exited)
# Note that this also deletes data containers you might have created.

#_____________________________________________________________________________________________________________